{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# initialize the training data augmentation object\n",
    "trainAug = ImageDataGenerator(rescale=1.0/255, \n",
    "                              shear_range = 0.2,\n",
    "                              zoom_range=0.2,\n",
    "                              horizontal_flip = True)\n",
    "\n",
    "# initialize the training generator\n",
    "trainGen = trainAug.flow_from_directory('/Users/nimo/Desktop/Machine_Learning/7/hw/dataset_train',  # path\n",
    "                                        target_size=(64, 64),  # all images will be resized to 64*64\n",
    "                                        batch_size=32,\n",
    "                                        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape=(32, 64, 64, 3), min=0.000, max=1.000\n"
     ]
    }
   ],
   "source": [
    "batchX, batchy = trainGen.next()\n",
    "print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the image shape of each training observation?\n",
    "    * (64, 64, 3)\n",
    "* How many total classes do we need to predict on? \n",
    "    * 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initial Classifier Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "# add the Conv2D layer\n",
    "classifier.add(Convolution2D(filters = 32,\n",
    "                             kernel_size = (3,3),\n",
    "                             input_shape = (64, 64, 3),\n",
    "                             activation = 'relu'))\n",
    "\n",
    "# add the MaxPooling2D layer      \n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))               \n",
    "    \n",
    "# add another Conv2D layer \n",
    "\n",
    "classifier.add(Convolution2D(filters = 64,\n",
    "                             kernel_size = (3,3),\n",
    "                             activation = 'relu'))\n",
    "\n",
    "# add another MaxPooling2D layer\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))  \n",
    "\n",
    "# add a Flatten layer \n",
    "classifier.add(Flatten())\n",
    "               \n",
    "# add a Dense layer    \n",
    "classifier.add(Dense(units = 128, activation = 'relu'))     \n",
    "               \n",
    "# add a final Dense layer\n",
    "classifier.add(Dense(units = 4, activation = 'softmax'))  \n",
    "               \n",
    "# compile   \n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 29, 29, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1605760   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 1,625,668\n",
      "Trainable params: 1,625,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3/3 [==============================] - 2s 454ms/step - loss: 1.4400 - accuracy: 0.3105\n",
      "Epoch 2/3\n",
      "3/3 [==============================] - 1s 363ms/step - loss: 0.9018 - accuracy: 0.6255\n",
      "Epoch 3/3\n",
      "3/3 [==============================] - 1s 351ms/step - loss: 0.4547 - accuracy: 0.9020\n"
     ]
    }
   ],
   "source": [
    "my_model = classifier.fit(trainGen, steps_per_epoch = 3, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "classifier.save('my_model.h5')\n",
    "print(\"Saved model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "dataset_test/C033.png\n",
      "dataset_test/1022.png\n",
      "dataset_test/4011.png\n",
      "dataset_test/1053.png\n",
      "dataset_test/6051.png\n",
      "dataset_test/4053.png\n",
      "dataset_test/C014.png\n",
      "dataset_test/6023.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([3]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([3]),\n",
       " array([1])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict\n",
    "import os, glob\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('my_model.h5')\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# test data path\n",
    "img_dir = \"dataset_test\" # Enter Directory of test set\n",
    "\n",
    "# iterate over each test image\n",
    "data_path = os.path.join(img_dir, '*g')\n",
    "files = glob.glob(data_path)\n",
    "\n",
    "# print the files in the dataset_test folder \n",
    "for f in files:\n",
    "    print(f)\n",
    "    \n",
    "# make a prediction and add to results \n",
    "data = []\n",
    "results = []\n",
    "for f1 in files:\n",
    "    img = image.load_img(f1, target_size = (64, 64))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis = 0)\n",
    "    data.append(img)\n",
    "    result = model.predict(img)\n",
    "    r = np.argmax(result, axis=1)\n",
    "    results.append(r)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category 1': 0, 'category 2': 1, 'category 3': 2, 'category 4': 3}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training data path\n",
    "train_img_dir = \"dataset_train\" \n",
    "\n",
    "# iterate over each training image\n",
    "train_data_path = os.path.join(train_img_dir, '*g')\n",
    "train_files = glob.glob(train_data_path)\n",
    "\n",
    "# check category labels in training_set\n",
    "trainGen.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = ['C033.png','1022.png','4011.png','1053.png','6051.png','4053.png','C014.png','6023.png']\n",
    "predict = [3, 0, 1, 0, 2, 2, 3, 1]\n",
    "actual = [3, 0, 2, 0, 1, 2, 3, 1]\n",
    "\n",
    "result_33 = {'img': img, 'predict': predict, 'actual': actual}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>predict</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C033.png</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1022.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4011.png</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1053.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6051.png</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4053.png</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C014.png</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6023.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        img  predict  actual\n",
       "0  C033.png        3       3\n",
       "1  1022.png        0       0\n",
       "2  4011.png        1       2\n",
       "3  1053.png        0       0\n",
       "4  6051.png        2       1\n",
       "5  4053.png        2       2\n",
       "6  C014.png        3       3\n",
       "7  6023.png        1       1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_33 = pd.DataFrame(data = result_33)\n",
    "df_33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 300ms/step - loss: 0.2734 - accuracy: 0.9583\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 453ms/step - loss: 0.3076 - accuracy: 0.9688\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 449ms/step - loss: 0.2038 - accuracy: 0.9375\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "1/1 [==============================] - 0s 362ms/step - loss: 0.4336 - accuracy: 0.8750\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 395ms/step - loss: 0.2582 - accuracy: 0.9062\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 403ms/step - loss: 0.2269 - accuracy: 0.9688\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Epoch 1/4\n",
      "2/2 [==============================] - 1s 329ms/step - loss: 0.1405 - accuracy: 0.9688\n",
      "Epoch 2/4\n",
      "2/2 [==============================] - 1s 336ms/step - loss: 0.1072 - accuracy: 0.9821\n",
      "Epoch 3/4\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.1503 - accuracy: 0.9464\n",
      "Epoch 4/4\n",
      "2/2 [==============================] - 1s 355ms/step - loss: 0.1200 - accuracy: 0.9643\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Epoch 1/5\n",
      "2/2 [==============================] - 1s 365ms/step - loss: 0.1553 - accuracy: 0.9643\n",
      "Epoch 2/5\n",
      "2/2 [==============================] - 1s 352ms/step - loss: 0.0826 - accuracy: 0.9821\n",
      "Epoch 3/5\n",
      "2/2 [==============================] - 1s 386ms/step - loss: 0.1362 - accuracy: 0.9643\n",
      "Epoch 4/5\n",
      "2/2 [==============================] - 1s 395ms/step - loss: 0.0495 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "2/2 [==============================] - 1s 366ms/step - loss: 0.0901 - accuracy: 0.9464\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Epoch 1/6\n",
      "2/2 [==============================] - 1s 346ms/step - loss: 0.0522 - accuracy: 0.9688\n",
      "Epoch 2/6\n",
      "2/2 [==============================] - 1s 222ms/step - loss: 0.0480 - accuracy: 0.9821\n",
      "Epoch 3/6\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 4/6\n",
      "2/2 [==============================] - 1s 393ms/step - loss: 0.0553 - accuracy: 0.9688\n",
      "Epoch 5/6\n",
      "2/2 [==============================] - 1s 330ms/step - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 6/6\n",
      "2/2 [==============================] - 1s 296ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Epoch 1/7\n",
      "3/3 [==============================] - 1s 294ms/step - loss: 0.0327 - accuracy: 0.9886\n",
      "Epoch 2/7\n",
      "3/3 [==============================] - 1s 336ms/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 3/7\n",
      "3/3 [==============================] - 1s 338ms/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 4/7\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 5/7\n",
      "3/3 [==============================] - 1s 273ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 6/7\n",
      "3/3 [==============================] - 1s 302ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 7/7\n",
      "3/3 [==============================] - 1s 261ms/step - loss: 0.0170 - accuracy: 0.9886\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Epoch 1/8\n",
      "3/3 [==============================] - 1s 313ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 2/8\n",
      "3/3 [==============================] - 1s 300ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 3/8\n",
      "3/3 [==============================] - 1s 327ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 4/8\n",
      "3/3 [==============================] - 1s 281ms/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 5/8\n",
      "3/3 [==============================] - 1s 347ms/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 6/8\n",
      "3/3 [==============================] - 1s 360ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 7/8\n",
      "3/3 [==============================] - 1s 306ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 8/8\n",
      "3/3 [==============================] - 1s 315ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Epoch 1/9\n",
      "3/5 [=================>............] - ETA: 0s - loss: 0.0087 - accuracy: 1.0000  WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 45 batches). You may need to use the repeat() function when building your dataset.\n",
      "5/5 [==============================] - 1s 172ms/step - loss: 0.0087 - accuracy: 1.0000\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Epoch 1/10\n",
      "3/5 [=================>............] - ETA: 0s - loss: 0.0053 - accuracy: 1.0000WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.0053 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# run other models \n",
    "my_model1 = classifier.fit(trainGen, steps_per_epoch = 1, epochs = 1)\n",
    "print('--------------------------------------------------------------------------------------------------')\n",
    "my_model2 = classifier.fit(trainGen, steps_per_epoch = 1, epochs = 2)\n",
    "print('--------------------------------------------------------------------------------------------------')\n",
    "my_model3 = classifier.fit(trainGen, steps_per_epoch = 1, epochs = 3)\n",
    "print('--------------------------------------------------------------------------------------------------')\n",
    "my_model4 = classifier.fit(trainGen, steps_per_epoch = 2, epochs = 4)\n",
    "print('--------------------------------------------------------------------------------------------------')\n",
    "my_model5 = classifier.fit(trainGen, steps_per_epoch = 2, epochs = 5)\n",
    "print('--------------------------------------------------------------------------------------------------')\n",
    "my_model6 = classifier.fit(trainGen, steps_per_epoch = 2, epochs = 6)\n",
    "print('--------------------------------------------------------------------------------------------------')\n",
    "my_model7 = classifier.fit(trainGen, steps_per_epoch = 3, epochs = 7)\n",
    "print('--------------------------------------------------------------------------------------------------')\n",
    "my_model8 = classifier.fit(trainGen, steps_per_epoch = 3, epochs = 8)\n",
    "print('--------------------------------------------------------------------------------------------------')\n",
    "my_model9 = classifier.fit(trainGen, steps_per_epoch = 5, epochs = 9)\n",
    "print('--------------------------------------------------------------------------------------------------')\n",
    "my_model10 = classifier.fit(trainGen, steps_per_epoch = 5, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.6752e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.2774e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.3663e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.9212e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 3.5886e-04 - accuracy: 1.0000\n",
      "Epoch 1\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 1.5861e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.9640e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.3524e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 1.6653e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.4016e-04 - accuracy: 1.0000\n",
      "Epoch 2\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 1.5115e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 2.7041e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.3171e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 1.4858e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.2437e-04 - accuracy: 1.0000\n",
      "Epoch 3\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 1.4469e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.4891e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 1.2635e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 1.3616e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.0897e-04 - accuracy: 1.0000\n",
      "Epoch 4\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.3834e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.3105e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 1.1994e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.2735e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.9368e-04 - accuracy: 1.0000\n",
      "Epoch 5\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.3181e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 2.1606e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 1.1284e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 1.2083e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 2.7863e-04 - accuracy: 1.0000\n",
      "Epoch 6\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.2550e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.0339e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 1.0584e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 1.1570e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 2.6431e-04 - accuracy: 1.0000\n",
      "Epoch 7\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.1965e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.9254e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 9.9235e-06 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.1143e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.5077e-04 - accuracy: 1.0000\n",
      "Epoch 8\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.1428e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.8313e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 9.3424e-06 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 1.0760e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.3811e-04 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# create a manual process to generate repeated training data\n",
    "n_batch = 5\n",
    "\n",
    "n_epochs= 9\n",
    "\n",
    "# loop through epochs\n",
    "for e in range(n_epochs):\n",
    "    print('Epoch', e)\n",
    "    batches = 0\n",
    "    # loop through batches\n",
    "    for x_batch, y_batch in trainAug.flow_from_directory('dataset_train', target_size=(64, 64), \n",
    "                                                              batch_size = 32, class_mode = 'categorical', seed = 74): \n",
    "        classifier.fit(x_batch, y_batch)\n",
    "        batches += 1\n",
    "        if batches >= n_batch:\n",
    "        # we need to break the loop by hand because the generator loops indefinitely\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 506ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 1\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7.5050e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 6.8319e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 2\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 9.9099e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 9.6391e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.9024e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.1014e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 3\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.3911e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 5.2380e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 5.9369e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.1409e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 4\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 6.2339e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 4.6177e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.9259e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 3.8438e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 5\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 4.2141e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 4.5802e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.8838e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 4.1864e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 8.0022e-04 - accuracy: 1.0000\n",
      "Epoch 6\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 2.9523e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 4.5398e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 1.5128e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 3.9455e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 5.7016e-04 - accuracy: 1.0000\n",
      "Epoch 7\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.3256e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 4.3308e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.3896e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.3691e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 4.6786e-04 - accuracy: 1.0000\n",
      "Epoch 8\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.9925e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 4.0011e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.3598e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 2.7722e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 4.1386e-04 - accuracy: 1.0000\n",
      "Epoch 9\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.7979e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.6329e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.3643e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.2820e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.8196e-04 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "n_batch = 5\n",
    "\n",
    "n_epochs= 10\n",
    "\n",
    "# loop through epochs\n",
    "for e in range(n_epochs):\n",
    "    print('Epoch', e)\n",
    "    batches = 0\n",
    "    # loop through batches\n",
    "    for x_batch, y_batch in trainAug.flow_from_directory('dataset_train', target_size=(64, 64), \n",
    "                                                              batch_size = 32, class_mode = 'categorical', seed = 74): \n",
    "        classifier.fit(x_batch, y_batch)\n",
    "        batches += 1\n",
    "        if batches >= n_batch:\n",
    "        # we need to break the loop by hand because the generator loops indefinitely\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Steps per Epoch</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.9643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.9464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.9886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Steps per Epoch  Epochs  Accuracy\n",
       "0                1       1    0.9583\n",
       "1                1       2    0.9688\n",
       "2                1       3    0.9688\n",
       "3                2       4    0.9643\n",
       "4                2       5    0.9464\n",
       "5                2       6    1.0000\n",
       "6                3       7    0.9886\n",
       "7                3       8    1.0000\n",
       "8                5       9    1.0000\n",
       "9                5      10    1.0000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a final dataframe that combines the accuracy across each combination.\n",
    "steps_per_epoch = [1,1,1,2,2,2,3,3,5,5]\n",
    "epochs = [1,2,3,4,5,6,7,8,9,10]\n",
    "accuracy = [0.9583,0.9688,0.9688,0.9643,0.9464,1.0000,0.9886,1.0000,1.0000,1.0000]\n",
    "model_result = {'Steps per Epoch': steps_per_epoch, \n",
    "                'Epochs': epochs, \n",
    "                'Accuracy': accuracy}\n",
    "df_model_result = pd.DataFrame(data = model_result)\n",
    "df_model_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conceptual Questions: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Discuss the effect of the following on accuracy and loss (train & test): \n",
    "    * Increasing the steps_per_epoch -- increase the accuracy and decrease the loss. \n",
    "    * Increasing the number of epochs -- increase the accuracy and decrease the loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Name two uses of zero padding in CNN.\n",
    "    * Control the shrinkage of dimension after applying filters larger than 1x1\n",
    "    * Avoid loosing information at the boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the use of a 1 x 1 kernel in CNN? \n",
    "    * Decrease the number of feature maps.\n",
    "    * Can be used for dimensionality reduction. (change the dimensionality in the filter space)\n",
    "    * Alter the depth of the input volume. \n",
    "    * Reduce computation cost in a network, by reducing the depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What are the advantages of a CNN over a fully connected DNN for this image classification problem?   \n",
    "    * Fully connected networks tend to perform less and aren’t good for feature extraction. Plus they have a higher number of weights to train that results in high training time. On the other hand, CNNs are trained to identify and extract the best features from the images for the problem at hand with relatively fewer parameters to train. The latter layers of a CNN are fully connected because of their strength as a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas\n",
    "import numpy\n",
    "import optparse\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, SimpleRNN, Masking, Embedding\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import OrderedDict\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the code\n",
    "dataframe = pandas.read_csv(\"dev-access.csv\", engine='python', quotechar='|', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to a numpy.ndarray type\n",
    "dataset = dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26773, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape of the data se\n",
    "dataset.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all rows and the 0th index as the feature data\n",
    "X = dataset[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all rows and index 1 as the target variable\n",
    "Y = dataset[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the predictors (including removing features that are not valuable, such as timestamp and source)\n",
    "for index, item in enumerate(X):\n",
    "    # Quick hack to space out json elements\n",
    "    reqJson = json.loads(item, object_pairs_hook=OrderedDict)\n",
    "    del reqJson['timestamp']\n",
    "    del reqJson['headers']\n",
    "    del reqJson['source']\n",
    "    del reqJson['route']\n",
    "    del reqJson['responsePayload']\n",
    "    X[index] = json.dumps(reqJson, separators=(',', ':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize our data, which just means vectorizing our text\n",
    "# given the data we will tokenize every character (thus char_level = True)\n",
    "tokenizer = Tokenizer(filters='\\t\\n', char_level=True)\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# we will need this later\n",
    "num_words = len(tokenizer.word_index)+1\n",
    "X = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad our data as each observation has a different length\n",
    "max_log_length = 1024\n",
    "X_processed = sequence.pad_sequences(X, maxlen=max_log_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the train set to be 75% of the data and the test set to be 25%\n",
    "np.random.seed(42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, Y, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train = X_train.astype('float64')\n",
    "y_train = y_train.astype('float64')\n",
    "X_test = X_test.astype('float64')\n",
    "y_test = y_test.astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model 1 - RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "Embedding(\n",
    "    input_dim = num_words,\n",
    "    output_dim = 32,\n",
    "    input_length = max_log_length)\n",
    "\n",
    "SimpleRNN(\n",
    "    units = 32,\n",
    "    activation = 'relu')\n",
    "\n",
    "Dense(\n",
    "    units = 1,\n",
    "    activation = 'relu')\n",
    "\n",
    "model.add(Embedding(input_dim = num_words,output_dim = 32,input_length = max_log_length))\n",
    "model.add(SimpleRNN(units = 32,activation = 'relu'))\n",
    "model.add(Dense(units = 1,activation = 'relu'))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1024, 32)          2016      \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,129\n",
      "Trainable params: 4,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "118/118 [==============================] - 46s 368ms/step - loss: 1.1249 - accuracy: 0.5085 - val_loss: 0.5706 - val_accuracy: 0.6488\n",
      "Epoch 2/3\n",
      "118/118 [==============================] - 32s 269ms/step - loss: 0.5584 - accuracy: 0.6715 - val_loss: 0.5564 - val_accuracy: 0.6657\n",
      "Epoch 3/3\n",
      "118/118 [==============================] - 29s 248ms/step - loss: 0.5038 - accuracy: 0.7426 - val_loss: 0.6489 - val_accuracy: 0.6313\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1652a4970>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the .fit() method to fit the model on the train data\n",
    "model.fit(X_train, y_train, validation_split = 0.25, epochs = 3, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 3s 57ms/step - loss: 0.6351 - accuracy: 0.6494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6351475715637207, 0.649387538433075]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the .evaluate() method to get the loss value & the accuracy value on the test data\n",
    "scores = model.evaluate(X_test, y_test, batch_size = 128)\n",
    "#print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model 2 - LSTM + Dropout Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential()\n",
    "\n",
    "model_2.add(Embedding(input_dim = num_words,output_dim = 32,input_length = max_log_length))\n",
    "model_2.add(LSTM(units = 64,recurrent_dropout = 0.5))\n",
    "model_2.add(Dropout(0.5))\n",
    "model_2.add(Dense(units = 1,activation = 'relu'))\n",
    "\n",
    "model_2.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1024, 32)          2016      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 26,913\n",
      "Trainable params: 26,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# print the model summary\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "118/118 [==============================] - 146s 1s/step - loss: 1.2488 - accuracy: 0.5275 - val_loss: 0.7807 - val_accuracy: 0.5789\n",
      "Epoch 2/3\n",
      "118/118 [==============================] - 131s 1s/step - loss: 0.6954 - accuracy: 0.6521 - val_loss: 0.6050 - val_accuracy: 0.6355\n",
      "Epoch 3/3\n",
      "118/118 [==============================] - 128s 1s/step - loss: 0.4647 - accuracy: 0.7858 - val_loss: 0.2622 - val_accuracy: 0.9653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1276b01f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the .fit() method to fit the model on the train data\n",
    "model_2.fit(X_train, y_train, validation_split = 0.25, epochs = 3, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 8s 152ms/step - loss: 0.2966 - accuracy: 0.9627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.29662224650382996, 0.9626531004905701]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the .evaluate() method to get the loss value & the accuracy value on the test data\n",
    "scores = model_2.evaluate(X_test, y_test, batch_size = 128)\n",
    "#print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Recurrent Neural Net Model 3: Build Your Own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model_3.add(Embedding(input_dim=num_words, input_length = max_log_length, output_dim=32))\n",
    "\n",
    "# Masking layer for pre-trained embeddings\n",
    "model_3.add(Masking(mask_value=0.0))\n",
    "\n",
    "# Recurrent layer\n",
    "model_3.add(LSTM(64, return_sequences=False, recurrent_dropout=0.5))\n",
    "\n",
    "# Fully connected layer\n",
    "model_3.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Dropout for regularization\n",
    "model_3.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model_3.add(Dense(units = 1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model_3.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 1024, 32)          2016      \n",
      "_________________________________________________________________\n",
      "masking_3 (Masking)          (None, 1024, 32)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 31,073\n",
      "Trainable params: 31,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "118/118 [==============================] - 191s 2s/step - loss: 0.6931 - accuracy: 0.5108 - val_loss: 0.6928 - val_accuracy: 0.5375\n",
      "Epoch 2/3\n",
      "118/118 [==============================] - 168s 1s/step - loss: 0.6928 - accuracy: 0.5312 - val_loss: 0.6927 - val_accuracy: 0.5243\n",
      "Epoch 3/3\n",
      "118/118 [==============================] - 194s 2s/step - loss: 0.6927 - accuracy: 0.5244 - val_loss: 0.6925 - val_accuracy: 0.5038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15abae490>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.fit(X_train, y_train, validation_split = 0.25, epochs = 3, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 11s 216ms/step - loss: 0.6926 - accuracy: 0.4886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6925635933876038, 0.488646537065506]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the .evaluate() method to get the loss value & the accuracy value on the test data\n",
    "scores = model_3.evaluate(X_test, y_test, batch_size = 128)\n",
    "#print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conceptual Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Explain the difference between the relu activation function and the sigmoid activation function.\n",
    "\n",
    "    * They are both activation function, which decides the output of a particular node in the neural network. Both function should differentiable, non-linear, and easy to handle.\n",
    "    * The Sigmoid output is defined by following equation: \n",
    "    $y(x) = \\frac{1}{1 + e^{-x}}$ \n",
    "        * Pros: Sigmoid is differentiable, non-linear, produces non-binary activations and it is bounded between (0,1).\n",
    "        * Cons: It can cause neural networks to suffer from the vanishing gradient problem since error is backpropagated through the layers and decreases dramatically with each hidden layer. The values are between 0 & 1 and it will be zero when value of the activation reaches 0 or 1 (the horizontal part of the curve). Also, it becomes increasingly more difficult for the neural network to adapt as the layers go up, thus imporve performance. \n",
    "        \n",
    "    * ReLU takes an input and directly outputs the input if positive and outputs 0 if negative. The function of Relu is:\n",
    "    $𝑦(𝑥)=𝑚𝑎𝑥(0,𝑥)$\n",
    "        * Pros: Relu doesn’t have a flat curve, it avoids vanishing gradient problem. Unlike sigmoid, reLU is called a piecewise function, because half of the output is linear (the positive output) while the other half is nonlinear. \n",
    "        * Cons: Relu is not differentiable at 0 and may result in exploding gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Describe what one epoch actually is (epoch was a parameter used in the .fit() method).\n",
    "    * The number of epochs is a hyperparameter of gradient descent that controls the number of complete passes through the training dataset. One epoch means that each sample in the training dataset has had an opportunity to update the internal model parameters. An epoch is comprised of one or more batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Explain how dropout works (you can look at the keras code and/or documentation) for (a) training, and (b) test data sets.\n",
    "    * Dropout works by randomly setting the outgoing edges of hidden units (neurons that make up hidden layers) to 0 at each update of the training phase. It is a technique used to prevent a model from overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Explain why problems such as this homework assignment are better modeled with RNNs than CNNs. What type of problem will CNNs outperform RNNs on?\n",
    "\n",
    "    * RNN is designed to work with sequence prediction problems, which were best described by the types of inputs and outputs supported. RNNs in general works very well with sequences of words and paragraphs (NLP), such as text data. Moreover, RNN is used in classification prediction problems, regression prediction problems, and generative models. However, RNNs are not appropriate for tabular datasets or image data input.\n",
    "    * However, CNNs were designed to map image data to an output variable. The benefit of using CNNs is the ability to develop an internal representation of a two-dimensional image. This allows the model to learn position and scale in variant structures in the data, which is important when working with images. More generally, CNNs work better with data that has a spatial relationship.\n",
    "    * In this case, our homework assignment are better modeled with RNNs than CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Explain what RNN problem is solved using LSTM and briefly describe how.\n",
    "    * The Long Short-Term Memory (LSTM) networks are capable of learning long-term dependencies. They are designed to avoid the long-term dependency problem. All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer. LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way. LSTM overcomes the problem that RNNs have -- the problems of training a recurrent network, and it in turn has been used on a wide range of applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
